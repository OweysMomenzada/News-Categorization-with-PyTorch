{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment <span style=\"color:red\">option Four</span> - News Categorization  using PyTorch \n",
    "Download the dataset from https://www.kaggle.com/uciml/news-aggregator-dataset and develop a news classification or categorization model. The dataset contain only titles of a news item and some metadata. The categories of the news items include one of: –<span  style=\"color:red\"> b</span> : business – <span  style=\"color:red\">t</span> : science and technology – <span  style=\"color:red\">e</span> : entertainment and –<span  style=\"color:red\">m</span> : health. \n",
    "\n",
    "1. Prepare training and test dataset: Split the data into training and test set (80% train and 20% test). Make sure they are balanced, otherwise if all <span  style=\"color:red\">b</span> files are on training, your model fails to predict <span  style=\"color:red\">t</span> files in test.\n",
    "2. Binary classification: produce training data for each two categories, such as <span  style=\"color:red\">b </span> and <span  style=\"color:red\"> t</span>, <span  style=\"color:red\">b</span> and <span  style=\"color:red\"> m</span>, <span  style=\"color:red\">e</span> and <span  style=\"color:red\">t</span> and so on. Evaluate the performance and report which categories are easier for the models.\n",
    "3. Adapt the Text Categorization PyTorch code (see above) and evaluate the performance of the system for these task\n",
    "4. Use a pre-trained embeddings and compare your result. When you use pre-ttrained mebeddings, you have to average the word embeddings of each tokens in ach document to get the unique representation of the document. DOC_EMBEDDING = (TOKEN1_EMBEDDING + ... + TOKENn_EMBEDDING). You can also use some of the <span  style=\"color:red\">spacy/FLAIR </span>document embedding methods\n",
    "6. Report the recall, precision, and F1 scores for both binary and multi-class classification.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiclassification FLASK API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "\n",
    "df = pd.read_csv('uci-news-aggregator.csv')\n",
    "\n",
    "del df['URL']\n",
    "del df['PUBLISHER']\n",
    "del df['STORY']\n",
    "del df['HOSTNAME']\n",
    "del df['TIMESTAMP']\n",
    "del df['ID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.CATEGORY.replace('b','business', inplace=True)\n",
    "df.CATEGORY.replace('t','science and technology', inplace=True)\n",
    "df.CATEGORY.replace('e','entertainment', inplace=True)\n",
    "df.CATEGORY.replace('m','health', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "lb_make = LabelEncoder()\n",
    "labels = lb_make.fit_transform(df.CATEGORY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['labels'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test =  train_test_split(df, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 75285 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "# The maximum number of most frequent words to be used.\n",
    "MAX_NB_WORDS = 10000\n",
    "# Max number of words in each row\n",
    "MAX_SEQUENCE_LENGTH = 150\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, lower=True)\n",
    "tokenizer.fit_on_texts(df['TITLE'].values)\n",
    "word2index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word2index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequence(texts):\n",
    "    batches = []\n",
    "    for text in texts:\n",
    "        layer = np.zeros(total_words,dtype=float)\n",
    "\n",
    "        for word in text_to_word_sequence(text):\n",
    "            try:\n",
    "                word2index[word.lower()]\n",
    "                layer[word2index[word.lower()]] += 1\n",
    "            except:\n",
    "                print(\"word is not included\")            \n",
    "        batches.append(layer)\n",
    "    \n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "learning_rate = 0.1\n",
    "num_epochs = 1\n",
    "batch_size = 2000\n",
    "display_step = 1\n",
    "total_words = len(word2index)\n",
    "hidden_size = 64 \n",
    "input_size = total_words\n",
    "num_classes = 4\n",
    "\n",
    "class ANN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(ANN, self).__init__()\n",
    "        self.layer_1 = nn.Linear(input_size,hidden_size, bias=True)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.layer_2 = nn.Linear(hidden_size, hidden_size, bias=True)\n",
    "        self.output_layer = nn.Linear(hidden_size, num_classes, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer_1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.layer_2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.output_layer(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We could observe that the loss is not decreasing under 0.08. Therefore, we earlystop after 3 Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ANN(\n",
       "  (layer_1): Linear(in_features=75285, out_features=64, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (layer_2): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (output_layer): Linear(in_features=64, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ANN(input_size, hidden_size, num_classes)\n",
    "model.load_state_dict(torch.load('mode.pth'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FLASK API - POST REQUESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_news = {'0':'business', '1':'entertainment', \n",
    "               '2':'health', '3':'science and technology'}\n",
    "\n",
    "def predict_news(text):\n",
    "    padded_text = pad_sequence([text])\n",
    "    torch_pad_text = torch.FloatTensor(padded_text)\n",
    "    output = model(torch_pad_text)\n",
    "    _, pred = torch.max(output.data, 1)\n",
    "    pred = str(int(pred))\n",
    "    \n",
    "    return labels_news[pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://localhost:9000/ (Press CTRL+C to quit)\n",
      "127.0.0.1 - - [28/Nov/2021 19:40:09] \"POST /api HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word is not included\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [28/Nov/2021 19:40:12] \"POST /api HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [28/Nov/2021 19:41:34] \"POST /api HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word is not included\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, render_template, request, abort, jsonify\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route(\"/api\", methods=[\"POST\"])\n",
    "def get_json():\n",
    "    if request.method == \"POST\":\n",
    "        if request.json:\n",
    "            request_json = request.json\n",
    "            if 'text' in request_json:\n",
    "                json_result = predict_news(request_json['text'])\n",
    "                return jsonify(json_result)\n",
    "            abort(400, 'JSON data missing text field.')\n",
    "        abort(415)\n",
    "    abort(405)\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    from werkzeug.serving import run_simple\n",
    "    run_simple('localhost', 9000, app)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
