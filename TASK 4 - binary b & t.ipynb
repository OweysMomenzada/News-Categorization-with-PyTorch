{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment <span style=\"color:red\">option Four</span> - News Categorization  using PyTorch \n",
    "Download the dataset from https://www.kaggle.com/uciml/news-aggregator-dataset and develop a news classification or categorization model. The dataset contain only titles of a news item and some metadata. The categories of the news items include one of: –<span  style=\"color:red\"> b</span> : business – <span  style=\"color:red\">t</span> : science and technology – <span  style=\"color:red\">e</span> : entertainment and –<span  style=\"color:red\">m</span> : health. \n",
    "\n",
    "1. Prepare training and test dataset: Split the data into training and test set (80% train and 20% test). Make sure they are balanced, otherwise if all <span  style=\"color:red\">b</span> files are on training, your model fails to predict <span  style=\"color:red\">t</span> files in test.\n",
    "2. Binary classification: produce training data for each two categories, such as <span  style=\"color:red\">b </span> and <span  style=\"color:red\"> t</span>, <span  style=\"color:red\">b</span> and <span  style=\"color:red\"> m</span>, <span  style=\"color:red\">e</span> and <span  style=\"color:red\">t</span> and so on. Evaluate the performance and report which categories are easier for the models.\n",
    "3. Adapt the Text Categorization PyTorch code (see above) and evaluate the performance of the system for these task\n",
    "4. Use a pre-trained embeddings and compare your result. When you use pre-ttrained mebeddings, you have to average the word embeddings of each tokens in ach document to get the unique representation of the document. DOC_EMBEDDING = (TOKEN1_EMBEDDING + ... + TOKENn_EMBEDDING). You can also use some of the <span  style=\"color:red\">spacy/FLAIR </span>document embedding methods\n",
    "6. Report the recall, precision, and F1 scores for both binary and multi-class classification.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary classification (Category: b & t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this notebook we train the model on a integer sequence based on the tokens (NOT PRETRAINED). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "\n",
    "df = pd.read_csv('uci-news-aggregator.csv')\n",
    "\n",
    "del df['URL']\n",
    "del df['PUBLISHER']\n",
    "del df['STORY']\n",
    "del df['HOSTNAME']\n",
    "del df['TIMESTAMP']\n",
    "del df['ID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>TITLE</th>\n",
       "      <th>CATEGORY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Titanfall Review Roundup</td>\n",
       "      <td>t</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Titanfall review: my buddy</td>\n",
       "      <td>t</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Respawn: Titanfall's server stability is in Mi...</td>\n",
       "      <td>t</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Titanfall's Resolution 'Likely' to be Increase...</td>\n",
       "      <td>t</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Xbox One Titanfall Bundle Release Date Tomorro...</td>\n",
       "      <td>t</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224306</th>\n",
       "      <td>224306</td>\n",
       "      <td>Cooley Advises The Honest Company on $70 Milli...</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224307</th>\n",
       "      <td>224307</td>\n",
       "      <td>Jessica Alba's Honest Co. Gets $70 Million in ...</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224308</th>\n",
       "      <td>224308</td>\n",
       "      <td>Jessica Alba is General Catalyst's newest bill...</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224309</th>\n",
       "      <td>224309</td>\n",
       "      <td>Jessica Alba's The Honest Company raises $70M</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224310</th>\n",
       "      <td>224310</td>\n",
       "      <td>Jessica Alba startup The Honest Co. raises $70...</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>224311 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         index                                              TITLE CATEGORY\n",
       "0            0                           Titanfall Review Roundup        t\n",
       "1            1                         Titanfall review: my buddy        t\n",
       "2            2  Respawn: Titanfall's server stability is in Mi...        t\n",
       "3            3  Titanfall's Resolution 'Likely' to be Increase...        t\n",
       "4            4  Xbox One Titanfall Bundle Release Date Tomorro...        t\n",
       "...        ...                                                ...      ...\n",
       "224306  224306  Cooley Advises The Honest Company on $70 Milli...        b\n",
       "224307  224307  Jessica Alba's Honest Co. Gets $70 Million in ...        b\n",
       "224308  224308  Jessica Alba is General Catalyst's newest bill...        b\n",
       "224309  224309      Jessica Alba's The Honest Company raises $70M        b\n",
       "224310  224310  Jessica Alba startup The Honest Co. raises $70...        b\n",
       "\n",
       "[224311 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_b = df.loc[df.CATEGORY == 'b']\n",
    "df_t = df.loc[df.CATEGORY == 't']\n",
    "df = df_t.append(df_b, ignore_index=True)\n",
    "\n",
    "df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.CATEGORY.replace('b','business', inplace=True)\n",
    "df.CATEGORY.replace('t','science and technology', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "lb_make = LabelEncoder()\n",
    "labels = lb_make.fit_transform(df.CATEGORY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['labels'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test =  train_test_split(df, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "business                  92604\n",
       "science and technology    86844\n",
       "Name: CATEGORY, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.CATEGORY.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    23363\n",
       "1    21500\n",
       "Name: labels, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.labels.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Because of memory issues we define 70000 as the maximum number of words considered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 47102 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# The maximum number of most frequent words to be used.\n",
    "MAX_NB_WORDS = 10000\n",
    "# Max number of words in each row\n",
    "MAX_SEQUENCE_LENGTH = 150\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, lower=True)\n",
    "tokenizer.fit_on_texts(df['TITLE'].values)\n",
    "word2index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word2index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "def get_batch(df,i,batch_size):\n",
    "    batches = []\n",
    "    results = []\n",
    "    texts = df.TITLE[i*batch_size:i*batch_size+batch_size]\n",
    "    categories = df.labels[i*batch_size:i*batch_size+batch_size]\n",
    "    \n",
    "    for text in texts:\n",
    "        layer = np.zeros(total_words,dtype=float)\n",
    "\n",
    "        for word in text_to_word_sequence(text):\n",
    "            try:\n",
    "                word2index[word.lower()]\n",
    "                layer[word2index[word.lower()]] += 1\n",
    "            except:\n",
    "                print(\"word is not included\")            \n",
    "        batches.append(layer)\n",
    "\n",
    "    for category in categories:\n",
    "        index_y = -1\n",
    "        if category == 0:\n",
    "            index_y = 0\n",
    "        elif category == 1:\n",
    "            index_y = 1\n",
    "        elif category == 2:\n",
    "            index_y = 2\n",
    "        else:\n",
    "            index_y = 3\n",
    "        results.append(index_y)\n",
    "\n",
    "    return np.array(batches),np.array(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequence(texts):\n",
    "    batches = []\n",
    "    for text in texts:\n",
    "        layer = np.zeros(total_words,dtype=float)\n",
    "\n",
    "        for word in text_to_word_sequence(text):\n",
    "            try:\n",
    "                word2index[word.lower()]\n",
    "                layer[word2index[word.lower()]] += 1\n",
    "            except:\n",
    "                print(\"word is not included\")            \n",
    "        batches.append(layer)\n",
    "    \n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "learning_rate = 0.1\n",
    "num_epochs = 3\n",
    "batch_size = 1000\n",
    "display_step = 1\n",
    "total_words = len(word2index)\n",
    "hidden_size = 64 \n",
    "input_size = total_words\n",
    "num_classes = 2\n",
    "\n",
    "class ANN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(ANN, self).__init__()\n",
    "        self.layer_1 = nn.Linear(input_size,hidden_size, bias=True)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.layer_2 = nn.Linear(hidden_size, hidden_size, bias=True)\n",
    "        self.output_layer = nn.Linear(hidden_size, num_classes, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer_1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.layer_2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.output_layer(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We could observe that the loss is not decreasing under 0.08. Therefore, we earlystop after 3 Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Step [4/179], Loss: 0.6174\n",
      "Epoch [1/3], Step [8/179], Loss: 0.4335\n",
      "Epoch [1/3], Step [12/179], Loss: 0.3415\n",
      "Epoch [1/3], Step [16/179], Loss: 0.2399\n",
      "Epoch [1/3], Step [20/179], Loss: 0.2127\n",
      "Epoch [1/3], Step [24/179], Loss: 0.2342\n",
      "Epoch [1/3], Step [28/179], Loss: 0.2475\n",
      "Epoch [1/3], Step [32/179], Loss: 0.2277\n",
      "Epoch [1/3], Step [36/179], Loss: 0.2329\n",
      "Epoch [1/3], Step [40/179], Loss: 0.2029\n",
      "Epoch [1/3], Step [44/179], Loss: 0.1981\n",
      "Epoch [1/3], Step [48/179], Loss: 0.1795\n",
      "Epoch [1/3], Step [52/179], Loss: 0.2235\n",
      "Epoch [1/3], Step [56/179], Loss: 0.2166\n",
      "word is not included\n",
      "Epoch [1/3], Step [60/179], Loss: 0.1610\n",
      "Epoch [1/3], Step [64/179], Loss: 0.1710\n",
      "Epoch [1/3], Step [68/179], Loss: 0.1800\n",
      "Epoch [1/3], Step [72/179], Loss: 0.1953\n",
      "Epoch [1/3], Step [76/179], Loss: 0.1638\n",
      "Epoch [1/3], Step [80/179], Loss: 0.1450\n",
      "Epoch [1/3], Step [84/179], Loss: 0.1534\n",
      "Epoch [1/3], Step [88/179], Loss: 0.1406\n",
      "Epoch [1/3], Step [92/179], Loss: 0.1433\n",
      "Epoch [1/3], Step [96/179], Loss: 0.1475\n",
      "Epoch [1/3], Step [100/179], Loss: 0.1614\n",
      "Epoch [1/3], Step [104/179], Loss: 0.1578\n",
      "Epoch [1/3], Step [108/179], Loss: 0.1647\n",
      "Epoch [1/3], Step [112/179], Loss: 0.1575\n",
      "Epoch [1/3], Step [116/179], Loss: 0.1835\n",
      "Epoch [1/3], Step [120/179], Loss: 0.1617\n",
      "Epoch [1/3], Step [124/179], Loss: 0.1513\n",
      "Epoch [1/3], Step [128/179], Loss: 0.1323\n",
      "Epoch [1/3], Step [132/179], Loss: 0.1492\n",
      "Epoch [1/3], Step [136/179], Loss: 0.1266\n",
      "Epoch [1/3], Step [140/179], Loss: 0.1250\n",
      "Epoch [1/3], Step [144/179], Loss: 0.1481\n",
      "Epoch [1/3], Step [148/179], Loss: 0.1358\n",
      "Epoch [1/3], Step [152/179], Loss: 0.1574\n",
      "Epoch [1/3], Step [156/179], Loss: 0.1402\n",
      "Epoch [1/3], Step [160/179], Loss: 0.1537\n",
      "Epoch [1/3], Step [164/179], Loss: 0.1520\n",
      "Epoch [1/3], Step [168/179], Loss: 0.1256\n",
      "Epoch [1/3], Step [172/179], Loss: 0.1119\n",
      "Epoch [1/3], Step [176/179], Loss: 0.1647\n",
      "Epoch [2/3], Step [4/179], Loss: 0.1114\n",
      "Epoch [2/3], Step [8/179], Loss: 0.1131\n",
      "Epoch [2/3], Step [12/179], Loss: 0.1027\n",
      "Epoch [2/3], Step [16/179], Loss: 0.1184\n",
      "Epoch [2/3], Step [20/179], Loss: 0.0993\n",
      "Epoch [2/3], Step [24/179], Loss: 0.1220\n",
      "Epoch [2/3], Step [28/179], Loss: 0.1152\n",
      "Epoch [2/3], Step [32/179], Loss: 0.1159\n",
      "Epoch [2/3], Step [36/179], Loss: 0.1299\n",
      "Epoch [2/3], Step [40/179], Loss: 0.1143\n",
      "Epoch [2/3], Step [44/179], Loss: 0.1105\n",
      "Epoch [2/3], Step [48/179], Loss: 0.0985\n",
      "Epoch [2/3], Step [52/179], Loss: 0.1221\n",
      "Epoch [2/3], Step [56/179], Loss: 0.1057\n",
      "word is not included\n",
      "Epoch [2/3], Step [60/179], Loss: 0.0893\n",
      "Epoch [2/3], Step [64/179], Loss: 0.1193\n",
      "Epoch [2/3], Step [68/179], Loss: 0.1031\n",
      "Epoch [2/3], Step [72/179], Loss: 0.1127\n",
      "Epoch [2/3], Step [76/179], Loss: 0.0972\n",
      "Epoch [2/3], Step [80/179], Loss: 0.0937\n",
      "Epoch [2/3], Step [84/179], Loss: 0.0968\n",
      "Epoch [2/3], Step [88/179], Loss: 0.0992\n",
      "Epoch [2/3], Step [92/179], Loss: 0.0912\n",
      "Epoch [2/3], Step [96/179], Loss: 0.0899\n",
      "Epoch [2/3], Step [100/179], Loss: 0.0972\n",
      "Epoch [2/3], Step [104/179], Loss: 0.0982\n",
      "Epoch [2/3], Step [108/179], Loss: 0.1049\n",
      "Epoch [2/3], Step [112/179], Loss: 0.1008\n",
      "Epoch [2/3], Step [116/179], Loss: 0.1150\n",
      "Epoch [2/3], Step [120/179], Loss: 0.0896\n",
      "Epoch [2/3], Step [124/179], Loss: 0.0945\n",
      "Epoch [2/3], Step [128/179], Loss: 0.0885\n",
      "Epoch [2/3], Step [132/179], Loss: 0.0995\n",
      "Epoch [2/3], Step [136/179], Loss: 0.0912\n",
      "Epoch [2/3], Step [140/179], Loss: 0.0626\n",
      "Epoch [2/3], Step [144/179], Loss: 0.0718\n",
      "Epoch [2/3], Step [148/179], Loss: 0.0879\n",
      "Epoch [2/3], Step [152/179], Loss: 0.1048\n",
      "Epoch [2/3], Step [156/179], Loss: 0.0724\n",
      "Epoch [2/3], Step [160/179], Loss: 0.0805\n",
      "Epoch [2/3], Step [164/179], Loss: 0.1104\n",
      "Epoch [2/3], Step [168/179], Loss: 0.0969\n",
      "Epoch [2/3], Step [172/179], Loss: 0.0817\n",
      "Epoch [2/3], Step [176/179], Loss: 0.0790\n",
      "Epoch [3/3], Step [4/179], Loss: 0.0725\n",
      "Epoch [3/3], Step [8/179], Loss: 0.0724\n",
      "Epoch [3/3], Step [12/179], Loss: 0.0957\n",
      "Epoch [3/3], Step [16/179], Loss: 0.0625\n",
      "Epoch [3/3], Step [20/179], Loss: 0.0645\n",
      "Epoch [3/3], Step [24/179], Loss: 0.0716\n",
      "Epoch [3/3], Step [28/179], Loss: 0.0894\n",
      "Epoch [3/3], Step [32/179], Loss: 0.0884\n",
      "Epoch [3/3], Step [36/179], Loss: 0.0953\n",
      "Epoch [3/3], Step [40/179], Loss: 0.0736\n",
      "Epoch [3/3], Step [44/179], Loss: 0.0797\n",
      "Epoch [3/3], Step [48/179], Loss: 0.0686\n",
      "Epoch [3/3], Step [52/179], Loss: 0.0825\n",
      "Epoch [3/3], Step [56/179], Loss: 0.0940\n",
      "word is not included\n",
      "Epoch [3/3], Step [60/179], Loss: 0.0617\n",
      "Epoch [3/3], Step [64/179], Loss: 0.0834\n",
      "Epoch [3/3], Step [68/179], Loss: 0.0728\n",
      "Epoch [3/3], Step [72/179], Loss: 0.0728\n",
      "Epoch [3/3], Step [76/179], Loss: 0.0660\n",
      "Epoch [3/3], Step [80/179], Loss: 0.0711\n",
      "Epoch [3/3], Step [84/179], Loss: 0.0712\n",
      "Epoch [3/3], Step [88/179], Loss: 0.0598\n",
      "Epoch [3/3], Step [92/179], Loss: 0.0686\n",
      "Epoch [3/3], Step [96/179], Loss: 0.0644\n",
      "Epoch [3/3], Step [100/179], Loss: 0.0665\n",
      "Epoch [3/3], Step [104/179], Loss: 0.0875\n",
      "Epoch [3/3], Step [108/179], Loss: 0.0682\n",
      "Epoch [3/3], Step [112/179], Loss: 0.0601\n",
      "Epoch [3/3], Step [116/179], Loss: 0.0859\n",
      "Epoch [3/3], Step [120/179], Loss: 0.0557\n",
      "Epoch [3/3], Step [124/179], Loss: 0.0596\n",
      "Epoch [3/3], Step [128/179], Loss: 0.0668\n",
      "Epoch [3/3], Step [132/179], Loss: 0.0654\n",
      "Epoch [3/3], Step [136/179], Loss: 0.0730\n",
      "Epoch [3/3], Step [140/179], Loss: 0.0498\n",
      "Epoch [3/3], Step [144/179], Loss: 0.0598\n",
      "Epoch [3/3], Step [148/179], Loss: 0.0674\n",
      "Epoch [3/3], Step [152/179], Loss: 0.0695\n",
      "Epoch [3/3], Step [156/179], Loss: 0.0629\n",
      "Epoch [3/3], Step [160/179], Loss: 0.0590\n",
      "Epoch [3/3], Step [164/179], Loss: 0.0729\n",
      "Epoch [3/3], Step [168/179], Loss: 0.0637\n",
      "Epoch [3/3], Step [172/179], Loss: 0.0658\n",
      "Epoch [3/3], Step [176/179], Loss: 0.0631\n"
     ]
    }
   ],
   "source": [
    "model = ANN(input_size, hidden_size, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  \n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_batch = int(len(train.TITLE)/batch_size)\n",
    "    for i in range(total_batch):\n",
    "        batch_x,batch_y = get_batch(train,i,batch_size)\n",
    "        articles = torch.FloatTensor(batch_x)\n",
    "        labels = torch.LongTensor(batch_y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(articles)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) % 4 == 0:\n",
    "            print ('Epoch [%d/%d], Step [%d/%d], Loss: %.4f'\n",
    "                   %(epoch+1, num_epochs, i+1, \n",
    "                     len(train.TITLE)/batch_size, loss.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#torch.save(ANN.state_dict(input_size, hidden_size, num_classes), 'mode.pth')\n",
    "\n",
    "# model = ANN(input_size, hidden_size, num_classes)\n",
    "# model.load_state_dict(torch.load('mode.pth'))\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divide and conquer\n",
    "\n",
    "Our dataset is huge and so is our dimension. Therefore, we use the divide and conquer approach, because we do not have enough memory to allocate the amount of data as numpy arrays\n",
    "\n",
    "\n",
    "##### note: this can take a little bit time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def div_and_conq(texts):\n",
    "    preds = []\n",
    "    texts = list(texts)\n",
    "    \n",
    "    for i in tqdm(range(len(texts))):\n",
    "        padded_text = pad_sequence([texts[i]])\n",
    "        torch_pad_text = torch.FloatTensor(padded_text)\n",
    "        output = model(torch_pad_text)\n",
    "        _, pred = torch.max(output.data, 1)\n",
    "        preds.extend(pred)\n",
    "        \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 44863/44863 [04:15<00:00, 175.52it/s]\n"
     ]
    }
   ],
   "source": [
    "X_test = div_and_conq(test.TITLE)\n",
    "Y_test = test.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy is 94.1734 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score,confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "acc_score = round(100*accuracy_score(X_test,Y_test),4)\n",
    "\n",
    "print(f\"Model accuracy is {acc_score} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "confusion_matrix = confusion_matrix(y_pred=X_test,y_true=Y_test)\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(6,6))\n",
    "sns.heatmap(confusion_matrix,annot=True,fmt=\"0.1f\",linewidths=1.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: [0.9390951  0.94466943]\n",
      "recall: [0.9497068  0.93306977]\n",
      "f1score: [0.94437114 0.93883377]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "\n",
    "precision, recall, fscore, _ = score(Y_test,X_test)\n",
    "\n",
    "print('precision: {}'.format(precision))\n",
    "print('recall: {}'.format(recall))\n",
    "print('f1score: {}'.format(fscore))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
